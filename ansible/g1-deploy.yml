- hosts: gluster_nodes
  become: yes
  any_errors_fatal: True
  vars:
    ha_base_dir: "/var/run/gluster/shared_storage/nfs-ganesha"
    ha_name: "gluster-ganesha-ha"


  tasks:
    - name: Build mntpath variable
      set_fact:
        mntpaths: "{{ mntpaths | default([]) }} + [{\"path\": \"/gluster/bricks/{{ item.thinLV }}\", \"lv_path\": \"/dev/mapper/{{ item.vg }}-{{ item.thinLV }}\"}]"
      with_items: "{{ backend_configuration }}"

    - name: Build arbiter_mntpath variable
      set_fact:
        arbiter_mntpaths: "{{ arbiter_mntpaths | default([]) }} + [{\"path\": \"/gluster/bricks/arbiter-{{ item.thinLV }}\", \"lv_path\": \"/dev/mapper/{{ item.vg }}-arbiter--{{ item.thinLV }}\"}]"
      with_items: "{{ backend_configuration }}"
      when: arbiter == True

     #NOTE: We assume all backend_devices are the same size and just check the first one
    - name: Set backend device name
      set_fact:
        backend_dev: "{{ backend_configuration[0]['device'].split('/')[2] }}"
      when: arbiter == True

    - name: Get backend device size
      set_fact:
        backend_dev_size: "{{ ansible_devices[backend_dev]['size'].split(' ')[0] }}"
      when: arbiter == True

    #Unfortunately it seems impossible to control the units output by ansible
    #facts, so we have to determine those units here and act accordingly
    - name: Get backend device size units
      set_fact:
        backend_dev_units: "{{ ansible_devices[backend_dev]['size'].split(' ')[1] }}"
      when: arbiter == True

    - name: Set conversion factor for MB
      set_fact:
        mb_conversion:  "{{ 1024|pow(3) if backend_dev_units == 'PB' else 1024|pow(2) if backend_dev_units == 'TB' else 1024 if backend_dev_units == 'GB' else 1 if backend_dev_units == 'MB' else (1/1024) if backend_dev_units == 'KB' }}"
      when: arbiter == True

    #NOTE: The additonal .009765625 factor keeps the result in base 2
    - name: Calculate arbiter brick size in MB
      set_fact:
        arbiter_size: "{{ (backend_dev_size|float * backend_configuration[0]['arbiter_size_factor']|float * mb_conversion|float * 0.009765625) }}"
      when: arbiter == True

    - name: Set PV data alignment for RAID
      set_fact:
        pv_dalign: "{{ diskcount|int * dalign|int }}"
      when: disktype == 'RAID'

    - name: Set PV data alignment for JBOD
      set_fact:
        pv_dalign: "{{ dalign }}"
      when: disktype != 'RAID'

    - name: Start glusterd service
      systemd:
        name: glusterd
        state: started

    - shell: /bin/bash -c 'echo "Creating LVM structures..." > {{ fifo }}'

    - name: Create data volume groups
      vg:
        action: create
        disks: "{{ item.device }}"
        vgname: "{{ item.vg }}"
        diskcount: "{{ diskcount }}"
        disktype: "{{ disktype }}"
        stripesize: "{{ dalign }}"
        # dalign value is passed through to pvcreate command
        # For data volumes, this is based on calculation above
        dalign: "{{ pv_dalign }}"
      with_items: "{{ backend_configuration }}"
      register: result
      failed_when: "result.rc != 0 and 'already exists' not in result.msg and 'is already in volume group' not in result.msg"

    - name: Create data thin pools
      lvol:
        vg: "{{ item.vg }}"
        lv: "{{ item.tp }}"
        size: "100%FREE"
        #NOTE: Hard-coding poolmetadatasize here assumes sufficiently large brick volumes
        opts: "--thin --chunksize {{ pv_dalign }}k --poolmetadatasize 16G"
      with_items: "{{ backend_configuration }}"

    - shell: /bin/bash -c 'echo "Creating cache partitions..." > {{ fifo }}'

    - set_fact:
        part_start: 0

    # NOTE: The GPT partition table is not being created correctly when
    #       included in the 'Create cache partitions' play below (bug?).
    #       The below two plays are a hack to work around this problem.
    #       This was relevant w/ ansible 2.3, but may no longer be an
    #       issue with 2.5.
    - name: Set cache disk labels
      parted:
        label: gpt
        state: present
        number: 1
        name: p1
        part_start: 0%
        part_end: 1%
        device: "{{ item }}"
        unit: s
      with_items: "{{ cache_devices }}"
      register: result

    - name: Get size of fast device
      set_fact:
        fast_dev_end: "{{ result['results'][0]['disk']['size']|int }}"

    - name: Delete temporary partition
      parted:
        state: absent
        device: "{{ item }}"
        number: 1
      with_items: "{{ cache_devices }}"

    #FIXME: We have a problem here between ansible versions.
    #About 2.3.2.0, it is important that label is either omitted or empty.
    #About 2.4-ish+, it is important that label is gpt
    - name: Create arbiter partitions
      parted:
        label: gpt
        #label: ''
        state: present
        device: "{{ item[0] }}"
        number: "{{ item[1]['id']|int + 1 }}"
        name: "p{{ item[1]['id']|int + 1 }}"
        flags: [ lvm ]
        part_start: "{{ item[1]['id']|int * arbiter_size|int + 1 }}MiB"
        part_end: "{{ (item[1]['id']|int + 1) * arbiter_size|int }}MiB"
        unit: s
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"
      when: arbiter == True
      register: result

    - set_fact:
        last_result: "{{ result['results']|last }}"
      when: arbiter == True

    - set_fact:
        last_part: "{{ last_result['partitions']|last }}"
      when: arbiter == True

    - name: Get end point of arbiter partition
      set_fact:
        arbiter_end: "{{ last_part['end']|int }}"
      when: arbiter == True

    - name: Calculate cache partition sizes
      set_fact:
        cache_part_size: "{{ ((fast_dev_end|int - arbiter_end|default(50)|int) - 50) / numdevices|int }}"

    - name: Define starting number for cache partitions
      set_fact:
        cache_part_start: "{{ 1 if arbiter != True else (1 + numdevices|int) }}"

    # We use the offsets for each cache partition to write 4MB of zeroes 
    # there. This is the only way to be sure there's no old lvm metadata 
    # anymore which would cause duplicate PVs etc. 
    - name: Clean up residual LVM metadata from all cache partition offsets
      shell: /bin/bash -c "/usr/bin/dd if=/dev/zero of={{ item[0] }} bs=1M count=4 seek={{ ( 512|int * (arbiter_end|default(50)|int + ((item[1]['id']|int * cache_part_size|int))) + 1|int) }} oflag=seek_bytes,direct"
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"

    - name: Create cache partitions
      parted:
        label: gpt
        #label: ''
        state: present
        device: "{{ item[0] }}"
        number: "{{ item[1]['id']|int + cache_part_start|int }}"
        name: "p{{ item[1]['id']|int + cache_part_start|int }}"
        flags: [ lvm ]
        part_start: "{{ (arbiter_end|default(50)|int + (item[1]['id']|int * cache_part_size|int)) + 1|int }}s"
        part_end: "{{ (arbiter_end|default(50)|int + ((item[1]['id']|int + 1) * cache_part_size|int))|int }}s"
        unit: s
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"

    - name: Get cache device partition naming convention
      shell: "lsblk -lp | grep part | grep -m 1 {{ item }} | awk '{print $1}'"
      register: result
      with_items: "{{ cache_devices }}"

      #NOTE: Assuming only one cache device; this will break with multiple cache devices
    - name: Isolate any partition prefix characters
      set_fact:
        part_prefix: '{{ result.results.0.stdout.split(cache_devices.0)[1] | default("") | regex_replace("[0-9]+$", "") }}'

      #NOTE: If we use the result of the create arbiter partitions play above, we can
      #      probably create the disks list using the dictionary there, which would be
      #      more accurate and would get rid of the hard-coded p here.
    - name: Create fast volume groups, initially only with arbiter PVs
      vg:
        action: create
        disks: "{{ item[0] }}{{ part_prefix }}{{ (item[1]['id']|int + 1) }}"
        vgname: "FAST{{ item[1]['vg'] }}"
        diskcount: "{{ diskcount }}"
        disktype: "{{ disktype }}"
        stripesize: "{{ dalign }}"
        # dalign value is passed through to pvcreate command
        # For cache volumes, this is same as other dalign
        dalign: "{{ dalign }}"
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"
      when: arbiter == True
      register: result
      failed_when: "result.rc != 0 and 'already exists' not in result.msg and 'is already in volume group' not in result.msg"

    - name: Create arbiter thin pools
      lvol:
        vg: "FAST{{ item['vg'] }}"
        lv: "arbiter-{{ item['tp'] }}"
        size: 100%FREE
        #TODO: Revisit metadata size
        #NOTE: dalign here is based on expectation that arbiter is on a fast JBOD device
        opts: "--thin --chunksize {{ dalign }}k --poolmetadatasize 4M --poolmetadataspare n"
      with_items: "{{ backend_configuration }}"
      when: arbiter == True

    - name: Set data thin pool zeroing mode off
      shell: bash -c "/sbin/lvchange --zero n {{ item.vg }}/{{ item.tp }}"
      with_items: "{{ backend_configuration }}"

    - name: Add cache partitions to fast VGs
      vg:
        action: create
        disks: "{{ item[0] }}{{ part_prefix }}{{ item[1]['id']|int + cache_part_start|int }}"
        vgname: "FAST{{ item[1]['vg'] }}"
        diskcount: "{{ diskcount }}"
        disktype: "{{ disktype }}"
        stripesize: "{{ dalign }}"
        # dalign value is passed through to pvcreate command
        # For cache volumes, this is same as other dalign
        dalign: "{{ dalign }}"
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"
      when: arbiter != True
      register: result
      failed_when: "result.rc != 0 and 'already exists' not in result.msg and 'is already in volume group' not in result.msg"

    - name: Add cache partitions to fast VGs
      shell: /bin/bash -c "/sbin/vgextend FAST{{ item[1]['vg'] }} {{ item[0] }}{{ part_prefix }}{{ item[1]['id']|int + cache_part_start|int }}"
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"
      when: arbiter == True

    - name: Create cache pools
      lvol:
        vg: "FAST{{ item[1]['vg'] }}"
        lv: "cpool{{ item[1]['id']|int + 1 }}"
        opts: "--type cache-pool --chunksize {{ pv_dalign }}k --poolmetadataspare n"
        pvs: "{{ item[0] }}{{ part_prefix }}{{ item[1]['id']|int + cache_part_start|int }}"
        size: "100%FREE"
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"

    - name: Deactivate fast VG volumes
      shell: /bin/bash -c "vgchange -an FAST{{ item['vg'] }}"
      with_items: "{{ backend_configuration }}"

    - name: Merge VGs
      shell: /bin/bash -c "vgmerge {{ item['vg'] }} FAST{{ item['vg'] }}"
      with_items: "{{ backend_configuration }}"

    - name: Reactivate VG volumes
      shell: /bin/bash -c "vgchange -ay {{ item['vg'] }}"
      with_items: "{{ backend_configuration }}"

    - name: Attach cache pool to thinpool tdata LV
      lv:
        action: convert
        lvtype: cache
        cachepool: "cpool{{ item['id']|int + 1}}"
        lvname: "{{ item['tp'] }}"
        vgname: "{{ item['vg'] }}"
      with_items: "{{ backend_configuration }}"
      register: result

    - name: Get thin pool size
      shell: "/sbin/lvs --units h | grep '^\ *{{ item['tp'] }}\ ' | awk '{print toupper($4)}'"
      with_items: "{{ backend_configuration }}"
      register: result

    - set_fact:
        volsize: "{{ result['results'][0]['stdout'] }}"

    - name: Create data logical volumes
      lv:
        action: create
        lvtype: thinlv
        poolname: "{{ item.tp }}"
        vgname: "{{ item.vg }}"
        lvname: "{{ item.thinLV }}"
        virtualsize: "{{ volsize }}"
      with_items: "{{ backend_configuration }}"

    - name: Create arbiter logical volumes
      lv:
        action: create
        lvtype: thinlv
        poolname: "arbiter-{{ item.tp }}"
        vgname: "{{ item.vg }}"
        lvname: "arbiter-{{ item.thinLV }}"
        virtualsize: "{{ arbiter_size }}M"
      with_items: "{{ backend_configuration }}"
      when: arbiter == True

    - debug: var=mntpaths verbosity=2

    - shell: /bin/bash -c 'echo "Creating and mounting file systems..." > {{ fifo }}'

    - name: Create data XFS filesystems
      filesystem:
        fstype: xfs
        dev: "{{ item.lv_path }}"
        opts: "-f -i size=512 -n size=8192 -d su={{ dalign }}k,sw={{ diskcount }}"
      with_items: "{{ mntpaths }}"

    - name: Create arbiter XFS filesystems
      filesystem:
        fstype: xfs
        dev: "{{ item.0.lv_path }}"
        opts: "-f -i size=512 -n size=8192 -d su={{ dalign }}k,sw={{ diskcount }}"
      with_nested:
        - "{{ arbiter_mntpaths }}"
        - "{{ backend_configuration }}"
      when: arbiter == True

    - name: Create the data brick mount points, skips if present
      file: path={{ item.path }} state=directory
      with_items: "{{ mntpaths }}"

    - name: Create the arbiter brick mount points, skips if present
      file: path={{ item.0.path }} state=directory
      with_nested:
        - "{{ arbiter_mntpaths }}"
        - "{{ backend_configuration }}"
      when: arbiter == True

    - name: Mount the data filesystems
      mount: name={{ item.path }} src={{ item.lv_path }} fstype=xfs
             opts="inode64,noatime,nodiratime" state=mounted
      with_items: "{{ mntpaths }}"

    - name: Mount the arbiter filesystems
      mount: name={{ item.0.path }} src={{ item.0.lv_path }} fstype=xfs
             opts="inode64,noatime,nodiratime" state=mounted
      with_nested:
        - "{{ arbiter_mntpaths }}"
        - "{{ backend_configuration }}"
      when: arbiter == True

    - name: Set the data mount SELinux context
      file: path={{ item.path }} setype=glusterd_brick_t
      with_items: "{{ mntpaths }}"

    - name: Set the arbiter mount SELinux context
      file: path={{ item.0.path }} setype=glusterd_brick_t
      with_nested:
        - "{{ arbiter_mntpaths }}"
        - "{{ backend_configuration }}"
      when: arbiter == True

    - shell: /bin/bash -c 'echo "Enabling glusterd services..." > {{ fifo }}'

    - name: Start and enable the glusterd service
      service:
        name: glusterd
        enabled: yes
        state: started

    - shell: /bin/bash -c 'echo "Configuring firewall..." > {{ fifo }}'

    - name: Open firewalld services for glusterfs
      firewalld:
        service: "{{ item }}"
        permanent: True
        immediate: True
        state: enabled
      with_items:
        - glusterfs
        - ntp
        #For nfs-ganesha
        - high-availability
        - nfs
        - rpc-bind
        - mountd
        - nlm
        - rquota
        #For samba
        - samba
          
    - name: Open firewalld ports for glusterfs
      firewalld:
        port: "{{ item }}"
        permanent: True
        immediate: True
        state: enabled
      with_items:
        - 111/tcp
        - 2049/tcp
        - 54321/tcp
        - 5900/tcp
        - 5900-6923/tcp
        - 5666/tcp
        - 16514/tcp
        - 662/tcp
        - 662/udp

    - shell: /bin/bash -c 'echo "Configuring storage network..." > {{ fifo }}'

    - include: g1-stor-net.yml

    - include: g1-etc-hosts.yml

    - include: g1-ntp.yml

    - name: Update resolv.conf
      template:
        src: g1-resolv-conf.j2
        dest: /etc/resolv.conf
        owner: root
        group: root
        mode: 0644

    - shell: /bin/bash -c 'echo "Testing storage network..." > {{ fifo }}'

    # This is actually necessary to kick-start the interface after bonding
    - name: Test reachability of host for peer probing
      delegate_to: 127.0.0.1
      run_once: true
    # The net_ping module apparently isn't in the RH ansible build :-/
    #  net_ping:
    #    dest: "{{ hostnames[0] }}"
    #    count: 5
      shell: /bin/ping -c 5 {{ hostnames[0] }}
      register: result
      until: result.rc == 0
      retries: 5
      delay: 2

    - shell: /bin/bash -c 'echo "Creating Gluster trusted storage pool..." > {{ fifo }}'

    #NOTE: The delegate_to host must match the master host or it is possible for a
    #      node to be excluded from the probing.
    - name: Create a Trusted Storage Pool
      delegate_to: "{{ hostnames[0] }}"
      run_once: true
      peer:
        action: probe
        hosts: "{{ hostnames }}"
        master: "{{ hostnames[0] }}"
      register: result
      until: result.rc == 0
      retries: 10
      delay: 3

    - name: Pause briefly
      pause:
        seconds: 5

    - shell: /bin/bash -c 'echo "Creating default Gluster volume..." > {{ fifo }}'

    - name: Build brick path for non-arbitrated volumes
      delegate_to: "{{ play_hosts[0] }}"
      run_once: true
      set_fact:
        brickstring_fuse: "{{ brickstring_fuse | default('') }},{{ item.1 }}:{{ item.0['path'] }}/{{ default_volname }}"
      with_nested:
        - "{{ mntpaths }} "
        - "{{ hostnames }}"
      when: arbiter != True

    - name: Build brick path for arbitrated volumes
      delegate_to: "{{ play_hosts[0] }}"
      run_once: true
      set_fact:
        brickstring_fuse: "{{ brickstring_fuse | default('') }},{{ item['node'] }}:{{ item['brick'] }}/{{ default_volname }}"
      with_items: "{{ replica_peers }}"
      when: arbiter == True

    - debug: var=brickstring_fuse verbosity=2
    - debug: var=brickstring_nfs verbosity=2
    - debug: var=brickstring_smb verbosity=2

    - name: Create Gluster volume for FUSE
      delegate_to: "{{ play_hosts[0] }}"
      run_once: true
      volume:
        action:         create
        volume:         "{{ default_volname }}"
        bricks:         "{{ brickstring_fuse }}"
        hosts:          "{{ play_hosts }}"
        replica:        "{{ replica }}"
        # Below values need to be quoted as they are appended to the command line
        replica_count:  "{{ replica_count }}"
        arbiter_count:  "{{ arbiter_count }}"
        disperse:       "{{ disperse }}"
        disperse_count: "{{ disperse_count }}"
        redundancy_count: "{{ redundancy_count }}"
        force:          "{{ force | default(False) }}"
      register: result
      failed_when: "result.rc != 0 and ('already exists' not in result.msg)"

    - shell: /bin/bash -c 'echo "Applying performance tuning..." > {{ fifo }}'

    - name: Set tuned profile
      shell: tuned-adm profile {{ tuned_profile }}

    - name: Set gluster volume parameters
      delegate_to: "{{ play_hosts[0] }}"
      run_once: true
      volume_set:
        action: set
        volume: "{{ default_volname }}"
        key: "{{ item.key }}"
        value: "{{ item.value }}"
      with_dict: "{{ gluster_vol_set }}"

    - shell: /bin/bash -c 'echo "Initializing NFS-Ganesha..." > {{ fifo}}'
      when: use_nfs == True

    - include: g1-nfs-ganesha.yml
      when: use_nfs == True

    - shell: /bin/bash -c 'echo "Initializing SMB & CTDB..." > {{ fifo}}'
      when: use_smb == True

    - include: g1-smb-ctdb.yml
      when: use_smb == True

    - shell: /bin/bash -c 'echo "Starting Gluster volumes..." > {{ fifo }}'

    - name: Starts a volume
      delegate_to: "{{ play_hosts[0] }}"
      run_once: true
      volume:
        action: start
        volume: "{{ item }}"
        force: "{{ force | default(False) }}"
      with_items:
        - "{{ default_volname }}"
      register: result
      failed_when: "result.rc != 0 and ('already started' not in result.msg)"
